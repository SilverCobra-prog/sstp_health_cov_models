{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPLvQcEdEt2d1p+Zivcnnom",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SilverCobra-prog/sstp_loneliness_classifiers/blob/main/loneliness_class_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ph7lQ52MHXul"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from textblob import TextBlob\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, classification_report"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download stopwords, tokenizers, and lemmatizer for text normalization\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "TPJ-4H8YMDga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the dataset\n",
        "# Load the dataset\n",
        "url = 'https://drive.google.com/uc?id={}'.format('12M_H4oziPEU5V0ee46wMbLvIPuEHj1HK')\n",
        "data = pd.read_csv(url)\n",
        "\n",
        "# Select relevant columns\n",
        "data = data.rename(columns={'human_label1': 'feelLonely'}).drop('human_label2', axis=1)\n",
        "data = data[[\"text\", \"feelLonely\"]]"
      ],
      "metadata": {
        "id": "te1osyQfHdHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for spelling correction\n",
        "def correct_spelling(text):\n",
        "    blob = TextBlob(text)\n",
        "    return str(blob.correct())\n",
        "\n",
        "\n",
        "# Remove punctuation from text\n",
        "def remove_punctuation(text):\n",
        "    return re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "data[\"text\"] = data[\"text\"].apply(remove_punctuation)\n",
        "\n",
        "# Function to convert text to lowercase\n",
        "def convert_to_lowercase(text):\n",
        "    return text.lower()\n",
        "\n",
        "# Function for text normalization\n",
        "def normalize_text(text):\n",
        "    # Remove punctuation\n",
        "    text = remove_punctuation(text)\n",
        "    # Correct spelling\n",
        "    text = correct_spelling(text)\n",
        "    # Convert to lowercase\n",
        "    text = convert_to_lowercase(text)\n",
        "    # Sentence segmentation\n",
        "    sentences = sent_tokenize(text)\n",
        "    # Tokenize each sentence and apply lemmatization\n",
        "    words = [lemmatizer.lemmatize(word) for sentence in sentences for word in word_tokenize(sentence) if word not in stop_words]\n",
        "    # Join the tokens back to text\n",
        "    return ' '.join(words)"
      ],
      "metadata": {
        "id": "HfA-NwHbLMN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Text vectorization\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(data[\"text\"])\n",
        "y = data[\"feelLonely\"]\n",
        "\n",
        "# Split the dataset into train, validation, and test sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
      ],
      "metadata": {
        "id": "YmHPZP0DIf0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert sparse matrices to dense arrays\n",
        "X_train = X_train.toarray()\n",
        "X_val = X_val.toarray()\n",
        "\n",
        "# Reshape the data for CNN\n",
        "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "X_val = X_val.reshape(X_val.shape[0], X_val.shape[1], 1)"
      ],
      "metadata": {
        "id": "MxVyFSO6JaTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the TextCNN model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=X_train.shape[1], output_dim=128, input_length=X_train.shape[1]))\n",
        "model.add(Conv1D(128, 5, activation='relu'))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Define early stopping criteria\n",
        "early_stopping = EarlyStopping(patience=3, restore_best_weights=True)"
      ],
      "metadata": {
        "id": "olT_-770JdIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32, callbacks=[early_stopping])\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test.reshape(X_test.shape[0], X_test.shape[1], 1), y_test)\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "ItKnklzhJe8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the confusion matrix\n",
        "sns.heatmap(cm, annot=True, cmap=\"Blues\", fmt=\"d\", xticklabels=[\"Not Lonely\", \"Lonely\"], yticklabels=[\"Not Lonely\", \"Lonely\"])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jyH-WrU9JjyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate classification metrics\n",
        "classification_metrics = classification_report(y_test, y_pred)\n",
        "print(classification_metrics)"
      ],
      "metadata": {
        "id": "0sGMSXTHJkdQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}