{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1940e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6f6e81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/grace/Documents/SSTP/all_loneliness_posts.csv',encoding='utf-8')\n",
    "df.drop(columns=['id','full_link'],inplace=True)\n",
    "df['feelLonely'] = df['feelLonely'].astype(int)\n",
    "df['text'] = df['text'].astype(str)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b153157e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stemmer = nltk.SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03d860b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a66735",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f363cfd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emojis(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                              u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                              u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                              u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                              u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                              u\"\\U00002702-\\U000027B0\"  # other miscellaneous symbols\n",
    "                              u\"\\U000024C2-\\U0001F251\"  # enclosed characters\n",
    "                              \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "def remove_hashtags(text):\n",
    "    pattern = r'(\\B#\\w+\\b)|(\\#\\w+\\b\\s*$)'\n",
    "    return re.sub(pattern, '', text)\n",
    "def remove_specialchars(text):\n",
    "    filtered_sent=[]\n",
    "    for char in text.split(' '):\n",
    "        if char=='&' or char=='$':\n",
    "            filtered_sent.append('')\n",
    "        else:\n",
    "            filtered_sent.append(char)\n",
    "    return ' '.join(filtered_sent)\n",
    "def remove_extraspace(text):\n",
    "    return re.sub('\\s\\s+','',text)\n",
    "def clean_text(text):\n",
    "    # Remove punctuations\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) \n",
    "    # Remove links\n",
    "    text = re.sub(r'http\\S+|www\\S+|\\S+\\.\\S+', '', text)\n",
    "    # Remove mentions\n",
    "    text = re.sub(r'@\\w+', '', text) \n",
    "    # Remove \\r\\n new line characters\n",
    "    text = text.replace('\\r\\n', ' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc09f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(text):\n",
    "    text = text.lower()\n",
    "    text = remove_emojis(text)  \n",
    "    text = remove_hashtags(text)                                                    \n",
    "    text = remove_specialchars(text)                                                    \n",
    "    text = remove_extraspace(text)                                                    \n",
    "    text = clean_text(text)                                                    \n",
    "    text = ' '.join(word for word in text.split() if word not in stop_words)    \n",
    "    text = ' '.join(lemmatizer.lemmatize(word) for word in text.split())                \n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f472fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned'] = df['text'].apply(preprocess_data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fef3a2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#removing the posts that contain 'removed' since they're deleted from the reddit\n",
    "df = df[df['cleaned'].str.contains('removed') == False]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c558eae2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install wordninja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aaa47de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wordninja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fa4b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split words that are combined together\n",
    "for i in df.index:\n",
    "    df.at[i,'cleaned'] = ' '.join(wordninja.split(df.at[i,'cleaned']))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef435930",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['feelLonely']==0].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5646b471",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['feelLonely']==1].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c3485b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wordcloud\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0838c7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#word cloud for not lonely posts\n",
    "wc1 = WordCloud(background_color=\"white\", max_words=50, stopwords=stop_words,\n",
    "                   max_font_size=40)\n",
    "wc1.generate(df.cleaned[df.feelLonely == 0].to_string())\n",
    "plt.imshow(wc1)\n",
    "plt.title('not lonely')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb6a200",
   "metadata": {},
   "outputs": [],
   "source": [
    "#word cloud for lonely posts\n",
    "wc2 = WordCloud(background_color=\"white\", max_words=50, stopwords=stop_words,\n",
    "                   max_font_size=40)\n",
    "wc2.generate(df.cleaned[df.feelLonely == 1].to_string())\n",
    "plt.imshow(wc2)\n",
    "plt.title('lonely')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e403498c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d28850d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#features\n",
    "X = df.cleaned\n",
    "#label\n",
    "y = df.feelLonely\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfa6d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data into training and test (80/20)\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,stratify=df.feelLonely)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03fc265",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644ec5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "\n",
    "X_train_cv = cv.fit_transform(X_train)\n",
    "X_test_cv = cv.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09696105",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fc9935",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_cv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2162a2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf = TfidfTransformer()\n",
    "X_train_tfidf = tfidf.fit_transform(X_train_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefcc8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7039827f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import f1_score ,accuracy_score, precision_score,recall_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import learning_curve, validation_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c59773d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#testing hyperparameters\n",
    "\n",
    "learning_rates = [0.08, 0.1, 0.16]\n",
    "max_depths = [6, 8, 10]\n",
    "num_estimators = [100,150,200,250]\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    for max_depth in max_depths:\n",
    "        for n_estimators in num_estimators:\n",
    "            print(learning_rate,max_depth,n_estimators)\n",
    "\n",
    "            pipe = Pipeline([\n",
    "                        ('bow', CountVectorizer()), \n",
    "                        ('tfidf', TfidfTransformer()),  \n",
    "                        ('model', xgb.XGBClassifier(\n",
    "                        objective='binary:logistic',\n",
    "                        learning_rate=learning_rate,\n",
    "                        max_depth=max_depth,\n",
    "                        n_estimators=n_estimators,\n",
    "                        eval_metric='logloss'))\n",
    "                 ])\n",
    "    \n",
    "            # Fit the pipeline with the data\n",
    "            pipe.fit(X_train, y_train)\n",
    "\n",
    "            y_pred = pipe.predict(X_test)   \n",
    "            report = classification_report(y_test, y_pred)\n",
    "            print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1007c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best params found above: learning rate-0.1, max tree depth-6, num trees-200\n",
    "# make xgb model with best params\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "            objective='binary:logistic',\n",
    "            learning_rate=0.1,\n",
    "            max_depth=6,\n",
    "            n_estimators=200,\n",
    "            use_label_encoder=False,\n",
    "            eval_metric='logloss')\n",
    "\n",
    "pipe = Pipeline([\n",
    "            ('bow', CountVectorizer()), \n",
    "            ('tfidf', TfidfTransformer()),  \n",
    "            ('model', xgb_model)\n",
    "        ])\n",
    "    \n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "y_pred = pipe.predict(X_test) \n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)\n",
    "print(f1_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb966e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline showing flow of data\n",
    "from sklearn import set_config\n",
    "set_config(display='diagram')\n",
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2c2439",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#confusion matrix\n",
    "cm = confusion_matrix(y_test,y_pred)\n",
    "display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0,1])\n",
    "display.plot()\n",
    "plt.title(\"confusion matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2936f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#overall metrics\n",
    "print('Accuracy Score:', round(accuracy_score(y_test, y_pred),2))\n",
    "print('Precision:', round(precision_score(y_test,y_pred),2))\n",
    "print('Recall:', round(recall_score(y_test,y_pred),2))\n",
    "print('F1 score:', round(f1_score(y_test,y_pred),2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
